{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ff50b6d-913e-4e79-896c-fa7f47afd4cf",
   "metadata": {},
   "source": [
    "# Tabular Data Formats: Comma Separated Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccad1d88-2527-4343-8388-7243c6cb4146",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.setup import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d31f17-b77d-467d-afb7-c2dae49e1934",
   "metadata": {},
   "source": [
    "Delimited text files, especially comma-separated values (CSV) files, are ubiquitous.  These are text files that put multiple values on each line, and separate those values with some semi-reserved character, such as a comma.  They are almost always the exchange format used to transport data between other tabular representations, but a great deal of data both starts and ends life as CSV, perhaps never passing through other formats.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2f7d62-02fb-41a4-ad81-3317b3e2b2d3",
   "metadata": {},
   "source": [
    "There are a great number of deficits in CSV files, but also some notable strengths.  CSV files are the format second most susceptible to structural problems.  All formats are generally equally prone to content problems, which are not tied to the format itself.  Spreadsheets like Excel are, of course, *by a very large margin* the worst format for every kind of data integrity concern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabb82ca-3481-4927-8e0b-ba8811f8a7a4",
   "metadata": {},
   "source": [
    "At the same time, delimited formats—or fixed-width text formats—are also almost the only ones you can easily open and make sense of in a text editor or easily manipulate using command-line tools for text processing.  Thereby delimited files are pretty much the only ones you can fix fully manually without specialized readers and libraries.  Of course, formats that rigorously enforce structural constraints *do avoid some* of the need to do this.  Many tabular formats **do** enforce structure and datatyping more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7a42af-4a0b-4ba2-ab18-692781ff2c52",
   "metadata": {},
   "source": [
    "One issue that you could encounter in reading CSV or other textual files is the actual character set encoding may not be the one you expect, or that is the default on your current system.  In this age of Unicode, this concern is diminishing, but only slowly, and archival files continue to exist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3845d6b-7aee-4edb-a8f4-2225149d4d5d",
   "metadata": {},
   "source": [
    "### Sanity Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4b53e5-1485-47d0-a495-d6fde07328d6",
   "metadata": {},
   "source": [
    "As a quick example, suppose you have just received a medium sized CSV file, and you want to make a quick sanity check on it. At this stage, we are concerned about whether the file is formatted correctly at all.  We can do this with command-line tools, even if most libraries are likely to choke on them (such as shown in the next code cell).  Of course, we could also use Python, R, or another general-purpose language if we just consider the lines as text initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00277aca-ae54-40f2-a562-cb80e0822796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use try/except to avoid full traceback in example\n",
    "try:\n",
    "    pd.read_csv('data/big-random.csv')\n",
    "except Exception as err:\n",
    "    print_err(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c2daaa-c656-41be-b3f6-891bc6f3b6a2",
   "metadata": {},
   "source": [
    "What went wrong there? Let us check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edfc698-fd52-43da-8e01-bf8b05cba7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the general size/shape of this file?\n",
    "wc = namedtuple('WordCount', 'lines words chars')\n",
    "with open('data/big-random.csv') as fh:\n",
    "    content = fh.read()\n",
    "    lines = len(content.splitlines())\n",
    "    words = len(content.split())\n",
    "    chars = len(content)\n",
    "wc(lines, words, chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10799bb1-cbd6-496a-a993-67d1023eefdc",
   "metadata": {},
   "source": [
    "Great! 100,000 rows; but there is some sort of problem on line 75 according to Pandas (and perhaps on other lines as well).  We might eyeball the file, but that could be time consuming.  Let's first check whether the lines have consistent numbers of commas on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63de13b-6a18-4534-97e6-1fc0d9aff104",
   "metadata": {},
   "outputs": [],
   "source": [
    "commas_per_line = Counter()\n",
    "with open('data/big-random.csv') as fh:\n",
    "    for line in fh:\n",
    "        commas_per_line[line.count(',')] += 1\n",
    "commas_per_line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62801b7-7fe7-4f2d-b783-5e4ddb00496c",
   "metadata": {},
   "source": [
    "So we have figured out already that 99,909 of the lines have the expected 5 commas.  But 46 have a deficit and 45 a surplus.  Perhaps we will simply discard the bad lines, but that is not altogether too many to consider fixing by hand, even in a text editor.  We need to make a judgement, on a per problem basis, about both the relative effort and reliability of automation of fixes versus manual approaches.  Let us take a look at a few of the problem rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e846ea-bf69-4ab8-a2cf-f09633b74c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_next = False\n",
    "for lineno, line in zip(range(1, 8_500), open('data/big-random.csv')):\n",
    "    if line.count(',') == 7:\n",
    "        print(lineno-1, previous, end='')\n",
    "        print(lineno, line, end='')\n",
    "        print_next = True\n",
    "    elif print_next:\n",
    "        print(lineno, line, end='')\n",
    "        print('--')\n",
    "        print_next = False\n",
    "    previous = line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b11717c-69ed-42a6-9c47-2767254f129d",
   "metadata": {},
   "source": [
    "<font color=\"darkcyan\">**Note**: In the book, and in my daily work, using bash for quick exploration is often quicker than Python or R.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64ea909-10b6-4339-a881-aa6da80313cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# grep -C1 -nP '^([^,]+,){7}' data/big-random.csv | head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d00061-24f1-49ee-a535-e71cf282c781",
   "metadata": {},
   "source": [
    "Looking at these lists of Italian words and integers of slightly varying number of fields does not immediately illuminate the nature of the problem.  We likely need more domain or problem knowledge.  However, given that fewer than 1% of rows are a problem, perhaps we should simply discard them for now.  If you do decide to make a modification such as removing rows, then versioning the data, with accompanying documentation of change history and reasons, becomes crucial to good data and process provenance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7e5cd7-902a-475b-8731-a604b547dc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [l.strip().split(',') \n",
    "         for l in open('data/big-random.csv') \n",
    "         if l.count(',') == 5]\n",
    "pd.DataFrame(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b283f8-45cc-4b89-afbe-528ab9164df4",
   "metadata": {},
   "source": [
    "In the code we managed, within Python, to read all rows without formatting problems.  We could also have used the `pd.read_csv()`, with `error_bad_lines=False` and some additional massaging to achieve a similar effect. Walking through it in plain Python gives you a better picture of what we exclude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fc6f85-4ad1-427d-a25f-cc3b79d4feba",
   "metadata": {},
   "source": [
    "### The Good, The Bad, and The Textual Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406fd8a4-4dca-4a6a-8579-4456e25189fb",
   "metadata": {},
   "source": [
    "Let us return to some virtues and deficits of CSV files.  Here when we mention CSV, we really mean any kind of delimited file.  And specifically, text files that store tabular data nearly always use a single character for a delimiter, and end rows/records with a newline (or carriage return and newline in legacy formats).  Other than commas, probably the most common delimiters you will encounter are tabs and the pipe character `|`.  However, nearly all tools are more than happy to use an arbitrary character."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b41999-729b-4776-acd9-cc0704b7b621",
   "metadata": {},
   "source": [
    "Fixed-width files are similar to delimited ones.  Technically they are different in that, although they are line oriented, they put each field of data in specific character positions within each line.  An example is used in a code cell below.  Decades ago, when Fortran and Cobol were more popular, fixed-width formats were more prevalent; my perception is that their use has diminished in favor of delimited files.  In any case, fixed-width textual data files have most of the same pitfalls and strengths as do delimited ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1392d61a-c20e-45de-9c91-e4f827bfced6",
   "metadata": {},
   "source": [
    "**The Bad**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0eef03-5c12-4ff8-bb4a-6234530ea9a9",
   "metadata": {},
   "source": [
    "Columns in delimited or flat files do not carry a data type, being simply text values.  Many tools will (optionally) make guesses about the data type, but these are subject to pitfalls.  Moreover, even where the tools accurately guess the broad type category (i.e. string vs. integer vs. real number) they cannot guess the specific bit length desired, where that matters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85f6067-ab6f-417c-b99f-1645405808fb",
   "metadata": {},
   "source": [
    "Likewise, the representation used for \"real\" numbers is not encoded—most systems deal with IEEE-754 floating-point numbers of some length, but occasionally decimals of some specific length are more appropriate for a purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12da5c0b-bd35-4f0a-bb14-b5ad7d8a530b",
   "metadata": {},
   "source": [
    "The most typical way that type inference goes wrong is where the initial records in some data set have an apparent pattern, but later records deviate from this.  The software library may infer one data type but later encounter strings that cannot be cast as such.  \"Earlier\" and \"later\" here can have several different meanings.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65128e35-f173-4a75-830b-aecf84c90c43",
   "metadata": {},
   "source": [
    "For many layouts, data frame libraries can guess a fixed-width format and infer column positions and data types (where it cannot guess, we could manually specify).  But the guesses about data types can go wrong.  For example, viewing the raw text, we see a fixed-width layout in `parts.fwf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dfcd48-7809-4192-a244-98bb7dfb6384",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in open('data/parts.fwf'):\n",
    "    print(line, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67400de6-00b5-4253-8804-707819fa2fe7",
   "metadata": {},
   "source": [
    "If we deliberately only read fewer rows of the `parts.fwf` file, Pandas will infer type `int64` for the `Part_No` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6c952e-9270-4f22-b29f-7e3d68e344b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_fwf('data/parts.fwf', nrows=3)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53d4773-4563-4436-af68-23091241024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14afedf5-5a6c-485a-b8c8-2bc8e2228749",
   "metadata": {},
   "source": [
    "However, if we read the entire file, Pandas does the \"right thing\" here: `Part_No` becomes a generic object, i.e. string. However, if we had a million rows instead, and the heuristics Pandas uses, for speed and memory efficiency, happened to limit inference to the first 100,000 rows, we might not be so lucky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a244583a-24a2-47ec-a105-0528a4f118f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_fwf('data/parts.fwf')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b854dce4-bdf4-41bf-b732-30642b0d33f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes  # type of `Part_No` changed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da5373a-1e39-425a-ab77-a9e4a6401852",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Delimited files—but not so much fixed-width files—are prone to escaping issues.  In particular, CSVs specifically often contain descriptive fields that sometimes contain commas within the value itself.  When done right, this comma should be escaped.  It is often not done right in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e53cff3-8c0c-43d4-9c51-4025c1658545",
   "metadata": {},
   "source": [
    "CSV is actually a family of different dialects, mostly varying in their escaping conventions.  Sometimes spacing before or after commas is treated differently across dialects as well. One approach to escaping is to put quotes around either every string value, or every value of any kind, or perhaps only those values that contain the prohibited comma.  This varies by tool and by the version of the tool.  Of course, if you quote fields, there is potentially a need to escape those quotes; usually this is done by placing a backslash before the quote character when it is part of the value.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61292400-ade5-484a-b910-74c5e56b27bc",
   "metadata": {},
   "source": [
    "An alternate approach is to place a backslash before those commas that are not intended as a delimeter but rather part of a string value (or numeric value that might be formatted, e.g. `$1,234.56`).  Guessing the variant can be a mess, and even single files are not necessarily self consistent between rows, in practice (often different tools or versions of tools have touched the data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a70ae6f-db6b-42d5-8b43-fb77c05359d4",
   "metadata": {},
   "source": [
    "The corresponding danger for fixed-width files, in contrast to delimited ones, is that values become too long.  Within a certain line position range you can have any codepoints whatsoever (other than newlines).  But once the description or name that someone thought would never be longer than, say, 20 characters becomes 21 characters, the format fails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863bb8b7-d460-4078-a3b7-25f8fe95fec4",
   "metadata": {},
   "source": [
    "<font color=\"darkcyan\">A special consideration arises around reading datetime formats.  Data frame libraries that read datetime values typically have an optional switch to parse certain columns as datetime formats.  Libraries such as Pandas support heuristic guessing of datetime formats; the problem here is that applying a heuristic to each of millions of rows can be *exceedingly slow*.  Where a date format is uniform, using a manual format specifier can make it several orders of magnitude faster to read.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82cd564-7b15-40f5-aa2b-4131dd19ec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(open('data/parts.tsv').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27697a50-99e0-4980-a9ff-0a036e5d82c4",
   "metadata": {},
   "source": [
    "The way Pandas parses dates heuristically uses the strange USA convention of of `m/d/y`.  The option `dayfirst=True` can modify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ace420c-a32f-4430-83b0-431ac1e38bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let Pandas make guesses for each row\n",
    "# VERY SLOW for large tables\n",
    "parts = pd.read_csv('data/parts.tsv', \n",
    "                    sep='\\t', parse_dates=['Date'])\n",
    "parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f216204-2422-4c26-8628-e9c215f323f8",
   "metadata": {},
   "source": [
    "We can verify that the dates are genuinely a datetime data type within the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95397313-c5ff-4687-a1d9-e89de59a1125",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84940e45-a636-4b90-9d7e-cf53f60eca66",
   "metadata": {},
   "source": [
    "We have looked at some challenges and limitations of delimited and fixed-width formats, let us consider their considerable advantages as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e3dd0b-9d6f-42f6-9ce9-b4d2f353376e",
   "metadata": {},
   "source": [
    "**The Good**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bb12b8-6614-4d55-b68a-107921a3eb9c",
   "metadata": {},
   "source": [
    "The biggest strength of CSV files, and their delimited or fixed-width cousins, is the ubiquity of tools to read and write them.  Every library dealing with data frames or arrays, across every programming language, knows how to handle them.  Most of the time the libraries parse the quirky cases pretty well.  Every spreadsheet program imports and exports as CSV.  Every RDBMS—and most non-relational databases as well—imports and exports as CSV.  Most programmers' text editors even have facilities to make editing CSV easier.  Python has a standard library module `csv` that processes many dialects of CSV (or other delimited formats) as a line-by-line record reader."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64580ea6-a64d-4ad9-9e97-e74b39ef2355",
   "metadata": {},
   "source": [
    "The lack of type specification is often a strength rather than a weakness.  For example, the part numbers mentioned a few pages ago may have started out always being integers as an actual business intention, but later on a need arose to use non-integer \"numbers.\"  With formats that have a formal type specifier, we generally have to perform a migration and copy to move old data into a new format that follows the loosened or revised constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4746630-60e6-446e-bfe7-4e233324d2b0",
   "metadata": {},
   "source": [
    "One particular case where a data type change happens especially often, in my experience, is with finite-width character fields.  Initially some field is specified as needing 5, or 15, or 100 characters for its maximum length, but then a need for a longer string is encountered later, and a fixed table structure or SQL database needs to be modified to accommodate the longer length.  Even more often—especially with databases—the requirement is underdocumented, and we wind up with a data set filled with truncated strings that are of little utility (and perhaps permanently lost data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60044b24-65ef-4956-8f8f-b5892d9fe3b0",
   "metadata": {},
   "source": [
    "Text formats in general are usually flexible in this regard. Delimited files—but not fixed-width files—will happily contain fields of arbitrary length.  This is similarly true of JSON data, YAML data<sup><i>config</i></sup>, XML data, log files, and some other formats that simply utilize text, often with line-oriented records.  In all of this, data typing is very loose and only genuinely exists in the data processing steps.  That is often a great virtue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbf656f-426f-447f-9d8d-0ec1ede252a7",
   "metadata": {},
   "source": [
    "<div id=\"config\"\n",
    "     style=\"display: inline-block; margin: 0 5% 0 5%; border-style: solid; border-width: 1px\">\n",
    "    <i>config</i><br/>\n",
    "YAML usually contains relatively short configuration information rather than *data* in the prototypic sense.  TOML is a similar format in this regard, as is the older INI format.  All of these are really intended for hand editing, and hence are usually of small size, even though good APIs for reading and writing their data are common.  While you <i>could</i> put a million records into any of these formats, you will rarely or never encounter that in practice.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8d3cfb-bbfe-4d8f-84ea-11df071b2bc0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "A related \"looseness\" of CSV and similar formats is that we often indefinitely aggregate multiple CSV files that follow the same informal schema.  Writing a different CSV file for each day, or each hour, or each month, of some ongoing data collection is very commonplace.  Many tools, such as Dask and **Spark** will seamlessly treat collections of CSV files (matching a *glob* pattern on the file system) as a single data set.  Of course, in tools that do not directly support this, manual concatenation is still not difficult.  But under the model of having a directory that contains an indefinite number of related CSV snapshots, presenting it as a single common object is helpful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
