{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a10f491-4114-4ab9-953a-9f8d6e0d8cce",
   "metadata": {},
   "source": [
    "# Tabular Data Formats: Tidying Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b80717-d88d-4cb6-9a4d-c3885d001d41",
   "metadata": {},
   "source": [
    "Check the Python version and load miscellaneous utility functions and classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9482bef9-ca8e-4f11-8762-b2e34dd43d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9287d5-da81-4040-9689-767de3a0cb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.setup import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7b04d9-e664-41ea-8a6f-c67f58c7a95e",
   "metadata": {},
   "source": [
    "## Types of Grime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8faca1-34ae-48ff-8f84-0a613da353c4",
   "metadata": {},
   "source": [
    "There are roughly two families of problems we find in data sets.  Not every problem neatly divides into these families, or at least it is not always evident which side something falls on without knowing the root cause.  But in a general way we can think of structural problems in the formatting of data versus content problems in the actual values recorded.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e9d52d-c91b-4caf-a97c-fb2bf42fe78c",
   "metadata": {},
   "source": [
    "On the structural branch a format used to encode a data set might simply \"put values in the wrong place\" in one way or another.  On the content side, the data format itself is correct, but implausible or wrong values have snuck in via flawed instruments, transcription errors, numeric overflows, or through other pitfalls of the recording process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80be95f-2a1b-40f0-84e6-d8ca93c3e35f",
   "metadata": {},
   "source": [
    "In the case of structural problems, we almost always need manual remediation of the data.  Exactly where the bytes that make up the data go wrong can vary enormously, and usually does not follow a pattern that lends itself to a single high-level description.  Often we have a somewhat easier time with the content problems, but at the same time they are more likely to be irremediable even with manual work.  Consider this small comma-separated value (CSV) data source, describing a 6th grade class:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013863ab-2931-449f-9b7e-7199e1bac5d4",
   "metadata": {},
   "source": [
    "```\n",
    "Student#,Last Name,First Name,Favorite Color,Age\n",
    "1,Johnson,Mia,periwinkle,12\n",
    "2,Lopez,Liam,blue,green,13\n",
    "3,Lee,Isabella,,11\n",
    "4,Fisher,Mason,gray,-1\n",
    "5,Gupta,Olivia,9,102\n",
    "6,,Robinson,,Sophia,,blue,,12\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7916acde-1994-4b3c-941f-6707561e3cbf",
   "metadata": {},
   "source": [
    "In a friendly way, we have a header line that indicates reasonable field names and provides a hint as to the meaning of each column.  Programmatically, we may not wish to work with the punctuation marks and spaces inside some field names, but that is a matter of tool convenience that we can address with the APIs (*application programming interfaces*; the functions and methods of a library) that data processing tools give us (perhaps by renaming them)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa82d3a-1880-4ad5-a214-4bcecaef2b38",
   "metadata": {},
   "source": [
    "Let us think about each record in turn.  \n",
    "* Mia Johnson, student 1, seems to have a problem-free record.  Her row has five values separated by four commas, and each data value meets our intuitive expectations about the data type and value domain.  The problems start hereafter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cddb9a-5bde-436f-b46e-c79f4f0d09dc",
   "metadata": {},
   "source": [
    "* Liam Lopez has too many fields in his row.  However, both columns 4 and 5 seem clearly to be in the lexicon of color names.  Perhaps a duplicate entry occurred or the compound color \"blue-green\" was intended.  Structurally the row has issues, but several plausible remediations suggest themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c341a5-0184-4662-9760-2496cdabe0e2",
   "metadata": {},
   "source": [
    "* Isabella Lee is perhaps no problem at all.  One of her fields is empty, meaning no favorite color is available.  But structurally, this row is perfectly fine for CSV format.  We will need to use some domain or problem knowledge to decide how to handle the missing value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65583c0-e169-422f-a880-2ba7e5f121f2",
   "metadata": {},
   "source": [
    "* Mason Fisher is perhaps similar to Isabella.  The recorded age of -1 makes no sense in the nature of \"age\" as a data field, at least as we usually understand it (but maybe the encoding intends something different).  On the other hand, -1 is one of several placeholder values used very commonly to represent missing data.  We need to know our specific problem to know whether we can process the data with a missing age, but many times we can handle that.  However, we still need to be careful not to treat the -1 as a plain value; for example, the mean, minimum, or standard deviation of ages might be thrown off by that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ea7a7d-45ac-4d62-9912-1cee3bf2e9d0",
   "metadata": {},
   "source": [
    "* Olivia Gupta starts to present a trickier problem.  Structurally her row looks perfect.  But '9' is probably not a string in our lexicon of color names.  And under our understanding of the data concerning a 6th grade class, we don't expect 102 year old students to be in it.  To solve this row, we really need to know more about the collection procedure and the intention of the data.  Perhaps a separate mapping of numbers to colors exists somewhere.  Perhaps an age of 12 was mistranscribed as 102; but also perhaps a 102 year old serves as a teaching assistant in this class and not only students are recorded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cfb8c5-600b-43f0-b3c3-6a58e2840d60",
   "metadata": {},
   "source": [
    "* Sophia Robinson returns us to what looks like an obvious structural error.  The row, upon visual inspection, contains perfectly good and plausible values, but they are separated by duplicate commas.  Somehow, persumably, a mechanical error resulted in the line being formatted wrongly.  However, most high-level tools are likely to choke on the row in an uninformative way, and we will probably need to remediate the issue more manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18579345-35e4-4783-996e-a102c196e911",
   "metadata": {},
   "source": [
    "We have a pretty good idea what to do with these six rows of data, and even re-entering them from scratch would not be difficult.  If we had a million rows instead, the difficulty would grow greatly, and would require considerable effort before we arrived at usable data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f1cbaf-c549-4d53-89a6-cb5e6805d7a6",
   "metadata": {},
   "source": [
    "## Nomenclature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1219be-4355-4818-8efa-0bdb56efe088",
   "metadata": {},
   "source": [
    "In this course I will use the terms *feature*, *field*, *measurement*, *column*, and occasionally *variable* more-or-less interchangeably.  Likewise, the terms *row*, *record*, *observation*, and *sample* are also near synonyms.  *Tuple* is used for the same concept when discussing databases (especially academically). In different academic or business fields, different ones of these terms are more prominent; and likewise different software tools choose among these. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fd1b3e-2d2f-407d-ad0a-9f9a6fd1ffdf",
   "metadata": {},
   "source": [
    "Conceptually, most data can be thought of as a number of occasions on which we measure various attributes of a common underlying *thing*.  In most tools, it is usually convenient to put these observations/samples each in a row; and correspondingly to store each of the measurements/features/fields pertaining to that thing in a column containing corresponding data for other comparable *things*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96db05d-2db9-447a-87a0-4e940fdd21f4",
   "metadata": {},
   "source": [
    "## Tidy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b71c04b-1ebc-47b1-a58f-d1ca9dc77f2c",
   "metadata": {},
   "source": [
    "Tidy data carefully separates variables (the columns of a table; also called features or fields) from observations (the rows of a table; also called samples).  At the intersection of these two, we find values, one data item (datum) in each cell.  Unfortunately, the data we encounter is often not arranged in this useful way, and it requires *normalization*.  In particular, what are really values are often represented either as columns or as rows instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e31c46-3e8b-44d3-90a4-ae8b0cc5ab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "students = pd.read_csv('data/students-scores.csv')\n",
    "students"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d0f2ae-ebf2-4044-b8d6-211f8ae11da1",
   "metadata": {},
   "source": [
    "This view of the data is easy for humans to read.  We can see trends in the scores each student received over several years of education.  Moreover, this format might lend itself to useful visualizations fairly easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87487c7e-14fa-4694-8b2d-4fe3a055bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic conversion of letter grades to numbers\n",
    "def num_score(x):\n",
    "    to_num = {'A+': 4.3, 'A': 4, 'A-': 3.7,\n",
    "              'B+': 3.3, 'B': 3, 'B-': 2.7,\n",
    "              'C+': 2.3, 'C': 2, 'C-': 1.7}\n",
    "    return x.map(lambda x: to_num.get(x, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed68a28-6816-4adc-9174-5c84f352d9e7",
   "metadata": {},
   "source": [
    "This next cell uses a \"fluent\" programming style that may look unfamiliar to some Python programmers.  I discuss this style in the section below on data frames.  The fluent style is used in many data science tools and languages. For example, this is typical Pandas code that plots the students' scores by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ad90e8-00d9-42e0-b2ad-9620bb05eff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(students\n",
    "     .set_index('Last Name')\n",
    "     .drop('First Name', axis=1)\n",
    "     .apply(num_score)\n",
    "     .T\n",
    "     .plot(title=\"Student score by year\")\n",
    "     .legend(bbox_to_anchor=(1, .75))\n",
    ")\n",
    "plt;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957f10e2-eede-4e50-9bfc-5997163af938",
   "metadata": {},
   "source": [
    "<font color=\"darkcyan\">**Question for students**: Should we review how that Pandas command was constructed?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0121dbaf-e4e2-46a2-8607-e937b868cf0a",
   "metadata": {},
   "source": [
    "This data layout exposes its limitations once the class advances to 7th grade, or if we were to obtain 3rd grade information.  To accommodate such additional data, we would need to change the number and position of columns, not simply add additional rows.  It is natural to make new observations or identify new samples (rows), but usually awkward to change the underlying variables (columns)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537d15dd-546a-47be-b3a2-75f0be2a3cfe",
   "metadata": {},
   "source": [
    "The particular class level (e.g. 4th grade) that a letter grade pertains to is, at heart, a value not a variable.  Another way to think of this is in terms of independent variables versus dependent variables.  Or in machine learning terms, features versus target.  In some way, the class level might correlate with or influence the resulting letter grade; perhaps the teachers at the different levels have different biases, or children of a certain age lose or gain interest in schoolwork, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea34917c-fa39-473f-b6c9-e059eee05a99",
   "metadata": {},
   "source": [
    "For most analytic purposes, this data would be more useful if we make it tidy (normalized) before further processing.  In Pandas, the `DataFrame.melt()` method can perform this tidying.  We pin some of the columns as `id_vars`, and we set a name for the combined columns as a variable and the letter grade as a single new column.  This Pandas method is slightly magical, and takes some practice to get used to.  The key thing is that it preserves data, simply moving it between column labels and data values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a863fa-4d9a-4e39-8498-1cd73ae5f400",
   "metadata": {},
   "outputs": [],
   "source": [
    "students.melt(\n",
    "    id_vars=[\"Last Name\", \"First Name\"], \n",
    "    var_name=\"Level\",\n",
    "    value_name=\"Score\"\n",
    ").set_index(['First Name', 'Last Name', 'Level'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ab958f-e740-483d-9cf8-30f5a4ff1c0b",
   "metadata": {},
   "source": [
    "Within the Tidyverse, specifically within the **tidyr** package, there is a function `pivot_longer()` that is similar to Pandas' `.melt()`.  The aggregation names and values have parameters spelled `names_to` and `values_to`, but the operation is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4f5443-e1ed-41a2-a758-8b1565be4488",
   "metadata": {},
   "source": [
    "The simple example above gives you a first feel for tidying tabular data.  To reverse the tidying operation that moves variables (columns) to values (rows), the `pivot_wider()` function in tidyr can be used.  In Pandas there are several related methods on DataFrames, including `.pivot()`, `.pivot_table()`, and `.groupby()` combined with `.unstack()`, which can create columns from rows (and do many other things too)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087c3215-a2cd-41e2-bcd8-3ba4e72d4548",
   "metadata": {},
   "source": [
    "Having looked at the idea of tidyness as a general goal for tabular, let us being looking at specific data formats, beginning with comma-separated values and fixed-width files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
